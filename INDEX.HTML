<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision Assist App</title>
    <style>
        video {
            width: 100%;
            height: auto;
            max-width: 600px;
            border: 2px solid #000;
        }
    </style>
</head>
<body>
    <h1>Vision Assist App</h1>
    <video id="videoFeed" autoplay></video>
    <p id="output"></p>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/axios/0.21.1/axios.min.js"></script>
    <script>
        async function startVideoFeed() {
            const constraints = {
                video: {
                    facingMode: { exact: "environment" }
                }
            };
            const videoElement = document.getElementById('videoFeed');

            try {
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                videoElement.srcObject = stream;
            } catch (error) {
                console.error('Error accessing camera:', error);
                alert('Error accessing camera. Please ensure you allow camera access.');
            }
        }

        async function captureFrame() {
            const videoElement = document.getElementById('videoFeed');
            const canvas = document.createElement('canvas');
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;
            const context = canvas.getContext('2d');
            context.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            return canvas.toDataURL('image/png').split(',')[1];  // Base64 encode the image
        }

        async function analyzeImage() {
            const base64Image = await captureFrame();
            try {
                const visionResponse = await axios.post(
                    'https://vision.googleapis.com/v1/images:annotate',
                    {
                        requests: [
                            {
                                image: {
                                    content: base64Image
                                },
                                features: [
                                    { type: "TEXT_DETECTION" }
                                ]
                            }
                        ]
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        params: {
                            key: 'AIzaSyCLNV_TwHkeVyxOD9m5ig8OMnO_bVbc9FU'
                        }
                    }
                );

                const detectedText = visionResponse.data.responses[0].fullTextAnnotation.text;
                askGPT(detectedText);

            } catch (error) {
                console.error('Error analyzing image:', error);
                alert('Error analyzing image.');
            }
        }

        async function askGPT(detectedText) {
            const prompt = `
                Analyze the following clinical vignette carefully. Think analytically before responding by accounting for all pertinent diagnostic and characteristic hallmark details listed within the vignette. Cross-reference each answer option presented to the vignette details and question asked, then compare them to each other. Create a reasoned list for why an answer option is most/best correct and why the others are incorrect. Double-check that this process has been followed before providing the final answer. Finally, return only the best letter option choice and the first 3-4 words of that answer choice.
                
                Clinical Vignette: ${detectedText}
            `;

            try {
                const gptResponse = await axios.post(
                    'https://api.openai.com/v1/chat/completions',
                    {
                        model: "gpt-4-turbo",
                        messages: [{ role: "user", content: prompt }]
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': `Bearer sk-YCSsxHTH_0-mbgJcUWYnf3VDIzvYnSahjp9FYRphJKT3BlbkFJ8p3neUlhhn6k9jDGb2q77wWNciciVc7j7YAP7FEf4A`
                        }
                    }
                );

                const aiMessage = gptResponse.data.choices[0].message.content.trim();
                document.getElementById('output').innerText = "AI Response: " + aiMessage;

                // Use Web Speech API to verbalize the response
                const utterance = new SpeechSynthesisUtterance(aiMessage);
                speechSynthesis.speak(utterance);

            } catch (error) {
                console.error('Error with GPT-4:', error);
                alert('Error with GPT-4.');
            }
        }

        // Start video feed when the page loads
        window.onload = () => {
            startVideoFeed();
            setInterval(analyzeImage, 13000);  // Capture and analyze a frame every 13 seconds
        };
    </script>
</body>
</html>
